版本更新说明

## v0.1.2
- 修复非标准openai格式的api端点重复尝试的问题，关联问题[#2](https://github.com/LiSheng2001/TransGPTex/issues/2)。目前遇到api的非标准错误将直接打印错误，并回退该段为原文以保全其他已经完成的翻译结果。
- 修复命令行设置系统prompt、用户prompt以及cot prompt的功能，关联问题[#4](https://github.com/LiSheng2001/TransGPTex/issues/4)。之前虽然预留了`--prompt_template`，但变量拼写错了导致一直没指定上。并且因为`cmd`对多行字符串支持较差，实际很难进行设置。

    因此，在新版本中使用了对多行字符串支持更好的`toml`配置文件进行配置，只需要在运行命令行的目录下配置`prompts.toml`文件并写入相关提示即可。更详细的效果可以参见文档`进阶使用`部分。
- 将异步请求模块的速率锁`rate_limiter`更改到任务执行时进行初始化，修复[问题](https://blog.csdn.net/whatday/article/details/106886621)。并且根据[方案](https://stackoverflow.com/questions/45600579/asyncio-event-loop-is-closed-when-getting-loop)修复`Windows`下协程报错"Asyncio Event Loop is Closed"的问题。

## v0.1.1
- 优化版本号显示，目前使用`tgtex -v`或`tgtex --version`即可显示当前版本号
- 修复特殊命令替换逻辑时使用`replace`替换导致短命令部分替换长命令而造成的解析错误。

    比如如果在先前匹配到`\renewcommand{\arraystretch}{0.9}`，如果后面有某行是`\renewcommand{\arraystretch}{0.9} %`，`replace`会将该行替换为`ls_replace_holder_0 %`从而导致后续解析出现问题。因为`find_scope`一定是从某行开头到另一行结尾，因此可以使用正则表达式加入这个先验，通过`rf"^{re.escape(target_scope)}$"`实现更精准的替换。


## v0.1.0
发布了0.1.0版本，在该版本中优化了0.16.0的很多问题：
1. 通过后处理修复`Doubao-1.5-pro-32k`模型喜欢在连字符"-"左右加空格的问题。如果这个功能影响了其他的tex命令，可以在命令行内添加`--not_fix_hyphen`禁用此功能。
2. 对`\def`、`\newcommand`等可能跨越多行的命令，不再使用之前的正则表达式方法，因为正则表达式没办法感知嵌套中括号、大括号的开始和结束。新版本使用了`find_scope`方法，通过栈记录中括号和大括号的开始与结束，从而精准查找这些命令的的作用域，实现更准确的占位符替换。
3. 优化公式正则表达式查找可能跨越多个公式，导致多个公式中间的文本无法翻译的问题。现在的正则表达式`r"^[ \t]*\\begin{equation\*?}(?:(?!\\end{equation\*?}).)*\\end{equation\*?}[ \t]*(?=\s*(?:\n|\Z))"`排除了中间包含结束符号的情况。
4. 参考`lm-evaluation-harness`，移除了之前的`qps`作为流量限制，转而使用了`num_concurrent`参数控制LLM API的并发数目。目前该值默认为100以保证翻译请求能被迅速处理。对于慢速API，可以通过`-num_concurrent n`设置较小的n以控制并发流量。

下一个版本计划将代码中的`print`切换为日志级别的log，更方便排查错误。

## v0.0.16
主要是填v0.0.10的一个坑：编译时会把`\modelname`解读为`\modelname的数据选择器`从而编译出现问题。在本版本应用了后处理，通过给它们前后添加空格来避免编译时的歧义。
虽然前后加空格可以减少编译错误，但也导致`\modelname`左右出现多余的空格。这在`\modelname`对应的字符串是英文时尚且不明显，但如果LLM将命令对应的文本也翻译为中文就会产生额外的空格，显得比较割裂。因此，在本版本选择直接闭合无参数命令，即`\modelname`改写为`\modelname{}`

## v0.0.15
deepseek-v3版本似乎在cot格式输出方面会遇到一些错误，因此在这个版本中加入cot格式错误后的重试尝试。
重试时如果温度继续为0.01，则可能重试后问题仍然存在，因此参考[openai论坛](https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api/172683)这个帖子设置默认temperature=0.2, top_p=0.1。因为latex翻译还是包含一些代码在里面的。

## v0.0.13
引入了思维链(cot)，默认是关闭的以节省token，毕竟cot很玄学，也不一定就比让LLM直接翻译好。开启之后，会让LLM根据要翻译的段落先进行思考，思考之后再进行正式翻译，以减少LLM翻译的生硬感，提高LLM的准确性。
目前的cot仅加入思考流程，未来可以考虑将论文标题和论文摘要当作上下文输入给模型，以让LLM感知到要翻译的论文片段对应的上下文信息。
使用`--use_cot`开启思维链式翻译。

## v0.0.12
1. 修复arxiv链接含有"www."时造成arxiv_id识别不正确的问题。
2. 对注释移除统一为移除整行注释从而减少编译时的问题。

## v0.0.11

v0.0.10的第2个修复理解有误，实际上在\author和\begin{table}包裹的命令中，对换行符很敏感，因为涉及到一些对齐问题。所以每次移除"%"注释必须完全移除。但对于需要的空行也不能随意更改，否则也会触发编译错误，如：
```latex
\begin{table}[t]
    \centering
    \begin{tabular}{p{100mm}}
    \toprule
    你是一个标签系统，为指令意图提供有用的标签，以区分有用的AI助手的指令。以下是一个指令：\\
    
    [begin]
    
    \{instruction\}
    
    [end]
    
    请提供粗粒度标签，例如“拼写和语法检查”和“角色扮演”，以识别上述指令的主要意图。
    你的回答应该是一个列表，包括标签的标题和每个标签的简要说明。
    你的回答必须严格遵循此JSON格式：[{"tag": str, "explanation": str}]。
    请用英语回答。 \\
    \bottomrule
    \end{tabular}
    \caption{\textsc{ChatGPT} 用于注释给定查询意图标签的提示模板。}
    \label{tab:tagging_prompt}
\end{table}
```

这里面\[begin\]前面的两个换行符不可以随意移除，否则会编译错误。

因此这版撤销v0.0.10的第2个改动，转为对\author和\begin{table}包裹的命令使用严格的单行注释清除。

另外，`\pdfToLatex=1`这个命令对于xelatex没啥用，可以直接移除掉。

## v0.0.10

修复了两个可能比较常见的bad case

1. 没有参数的命令和中文粘连的问题

比如，翻译后的文本:
```latex
基于这一观察，我们提出了一种基于\modelname的数据选择器，从开源数据集中选择6K个多样且复杂的样本，并在\modelname选择的数据上微调模型。
最终的模型\lmname在\textsc{MT-Bench}评估的基础上，超越了基于大规模SFT数据的开源模型，呼应了查询多样性和复杂性的重要性。
```
看着没啥问题，但编译时会把`\modelname`解读为`\modelname的数据选择器`从而编译出现问题。在本版本应用了后处理，通过给它们前后添加空格来避免编译时的歧义。处理后的文本:
```latex
基于这一观察，我们提出了一种基于 \modelname 的数据选择器，从开源数据集中选择6K个多样且复杂的样本，并在 \modelname 选择的数据上微调模型。
最终的模型 \lmname 在 \textsc{MT-Bench} 评估的基础上，超越了基于大规模SFT数据的开源模型，呼应了查询多样性和复杂性的重要性。
```

2. `\\`与注释移除带来的异常空格问题

出于节省token的考虑，在预处理时移除了注释，但为了保留一部分可读性便于后面编辑，只是把注释那行变成了空白行。这在大多数用例里都没有问题，但似乎和`\\`同时使用会出现编译问题。例如:
```latex
\author{Author 1 \& Author 2 \& Author 3 \&
机构名 \\
% 原来有的注释
}
```
预处理后变成:
```latex
\author{Author 1 \& Author 2 \& Author 3 \&
机构名 \\

}
```
这样居然会报错了，我们在这个版本用正则表达式手动移除`\\`后多余的换行符来减少编译错误。

## v0.0.9

该版本主要进行了如下改动：
- 针对gpt-4o-mini会在\end{abstract}后补充\end{document}的特定case，进行了一些启发式方法进行预防。
- 对于一些直接提供了`.bbl`而没有提供`.bib`文件的latex项目进行了编译优化。跳过`bibtex`编译并直接把`.bbl`目录复制到输出目录。

## v0.0.8

推荐切换到gpt-4o-mini，速度快，价格没有那么夸张而且效果好。

另外，还进行了一些常规更新和修改：
- arxiv论文翻译时默认用论文标题作为文件夹路径。
- 默认切换到gpt-4o-mini和openai官方api端点。
- 将默认qps降低为3。


## v0.0.7

修复请求时产生的错误，具体如下：

- 修复触发`429限额错误`时将`status_code`解析成字符串而不是整数的错误。
- 修复重试时失败任务下标错误。
- 修复因触发LLM风控，导致意外退出的问题。目前解决方案是如果触发LLM风控，则该部分回退到Latex原文。触发风控的代码参考deepseek设置为400。
- 将重试轮数从3轮更改为10轮，降低因模型qps低而导致整体结果不可用的概率。

## v0.0.6

修改了latex项目翻译的逻辑。之前是按tex文件依次调用API进行翻译的，因此在子tex文件较多而且文件内容较短时的时候会接近串行请求。目前修改为先获取整个项目要翻译的tex片段之后统一请求，请求完了之后再进行相应的后处理，对于子tex文件较多的项目可以缩短等待时间。(速度优化)

## v0.0.5

新增了更多命令的吸收，并且合并了相邻占位符，希望能够带来更好的翻译输出。

- 新增吸收\def和\newcommand命令，避免LLM错误修改导致编译问题。
- 合并相邻的占位符，减少LLM复制大块占位符时产生的幻觉。
- 新增处理\`\`\`的逻辑，可以减少deepseek等模型无中生有\`\`\`产生的问题。